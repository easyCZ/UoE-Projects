Student: s1115104
Date: 24-10-2015

Output directory: /user/s1115104/data/output/exc-cw2/

###########################################################
# Task 1
###########################################################

Approach:
    1. Count words in each line and ouput a triple (word, filename, word_count)
    2. Partition output by word, sort primarily by word, secondarily by filename (alphabetically)
    3. Combiner accumulates counts by filename
    4. Reducer produces total counts for all files given a name and also outputs counts for each file

Document Sorting:
    Documents output are sorted alphabeitcally, thefeore d11.txt comes before d2.txt.

Task 1 code begin
Run:
#!/usr/bin/bash
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=16 \
 -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
 -D stream.num.map.output.key.fields=1 \
 -D num.key.fields.for.partition=1 \
 -D mapreduce.partition.keypartitioner.options=-k1,1 \
 -D mapreduce.partition.keycomparator.options="-k1,1 -k2,2" \
 -D mapreduce.map.output.key.field.separator=" " \
 -input /data/assignments/ex2/task1/large/ \
 -output /user/s1115104/data/output/exc-cw2/s1115104_task_1.out \
 -mapper mapper.py \
 -file mapper.py \
 -combiner combiner.py \
 -file combiner.py \
 -reducer reducer.py \
 -file reducer.py \
 -jobconf mapred.job.name="Inverted Index L" \
 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner

########################################################
#!/usr/bin/python
# mapper.py
import os
import sys
from collections import Counter

filename = os.environ['mapreduce_map_input_file'].split('/')[-1]

for line in sys.stdin:
    words = line.strip().split()
    if words:
        counter = Counter(words)

        for (word, count) in counter.iteritems():
            print("{0} {1} {2}".format(word, filename, count))

########################################################
#!/usr/bin/python
# combiner.py
import os
import sys

last_word = None
last_filename = None
accumulator = 0

# Input is received sorted by word and by filename
for line in sys.stdin:
    line = line.strip()

    word, filename, count = line.split(' ', 2)

    if last_word is not None and word != last_word:
        print("{0} {1} {2}".format(last_word, last_filename, accumulator))
        accumulator = 0

    last_word = word
    last_filename = filename
    accumulator += int(count)


if last_word is not None:
    print("{0} {1} {2}".format(last_word, last_filename, accumulator))
########################################################
#!/usr/bin/python
# reducer.py
import sys

word_counter = 0
last_word = None
word_documents = []


def write(keyword, total, documents):
    docs = ", ".join(["(%s, %d)" % (doc, count) for doc, count in documents])
    docs = "{%s}" % docs
    print("{0} : {1} : {2}".format(keyword, total, docs))

# Input is sorted primarily by word, secondarily by
# document name. Single pass is required without doing any
# sorting.
#
# Expected input:
#   also        d1.txt  13
#   also        d3.txt  8
#   also        d4.txt  3
#   altered     d5.txt  1
#   althorp     d5.txt  2
for line in sys.stdin:
    word, doc, count = line.split(' ', 2)

    if last_word is not None and last_word != word:
        # Cleanup after last block
        write(last_word, word_counter, word_documents)
        # Start new block
        word_documents = []
        word_counter = 0

    count = int(count)

    last_word = word
    word_counter += count
    word_documents.append((doc, count))

if last_word is not None:
    write(last_word, word_counter, word_documents)

Task 1 code end

Task 1 results begin
"'Ah, : 2 : {(d15.txt, 2)}	
"'Bad : 1 : {(d3.txt, 1)}	
"'Do : 6 : {(d15.txt, 5), (d2.txt, 1)}	
"'It : 5 : {(d15.txt, 4), (d5.txt, 1)}	
"'Listen, : 1 : {(d15.txt, 1)}	
"'Marriage? : 1 : {(d15.txt, 1)}	
"'Permit : 1 : {(d15.txt, 1)}	
"'To : 2 : {(d15.txt, 1), (d2.txt, 1)}	
"'Would : 1 : {(d15.txt, 1)}	
"According : 1 : {(d3.txt, 1)}	
Task 1 results end


###########################################################
# Task 2
###########################################################

Approach:
    Re-use index from part 1
    1. Compute index for the relevant files
    2. Use only mapper job
    2. Load the required terms file (terms.txt)
    3. For each keyword in terms.txt, compute tf-idf given the index
    4. Output only relevant keyword and their scores

Params:
    Mapper accepts two params, the total number of files and the document we are
    looking to caluclate tf-idf for.

Notes:
    Despite running it against only one file, the solution supports N documents.

Task 2 code begin
Run:
Uses code from Task1 to build the index.

#!/usr/bin/bash
export EXC2_2_INPUT=/data/assignments/ex2/task1/large/d1.txt
export EXC2_2_INDEX=/user/s1115104/data/output/exc-cw2/s1115104_task_2.out/index
export EXC2_2_OUTPUT=/user/s1115104/data/output/exc-cw2/s1115104_task_2.out/score

# Clean up last run
rm ./terms.txt

# Fetch terms to look for
hdfs dfs -get /data/assignments/ex2/terms.txt ./terms.txt

# Build index
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=5 \
 -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
 -D stream.num.map.output.key.fields=1 \
 -D num.key.fields.for.partition=1 \
 -D mapreduce.partition.keypartitioner.options=-k1,1 \
 -D mapreduce.partition.keycomparator.options="-k1,1 -k2,2" \
 -D mapreduce.map.output.key.field.separator=" " \
 -input $EXC2_2_INPUT \
 -output $EXC2_2_INDEX \
 -mapper ../task1/mapper.py \
 -file ../task1/mapper.py \
 -combiner ../task1/combiner.py \
 -file ../task1/combiner.py \
 -reducer ../task1/reducer.py \
 -file ../task1/reducer.py \
 -jobconf mapred.job.name="Inverted Index L" \
 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner

# Calculate score
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=0 \
 -input $EXC2_2_INDEX \
 -output $EXC2_2_OUTPUT \
 -mapper "mapper.py `hdfs dfs -ls $EXC2_2_INPUT | grep -v "_SUCCESS\|Found" | wc -l` d1.txt"\
 -file mapper.py \
 -jobconf mapred.job.name="TF-IDF L" \
 -file terms.txt

hdfs dfs -cat $EXC2_2_OUTPUT/part-* | head -n 10 > results.txt
########################################################
#!/usr/bin/python
# mapper.py
import os
import sys
import math

# Requires the number of files to map to be part of the input
DOCUMENT_COUNT = int(sys.argv[1])
FILENAME = sys.argv[2]
TERMS = set()

# Build dictionary
with open('terms.txt') as f:
    for term in f:
        term = term.strip()
        TERMS.add(term)

# Input pattern:
# "And : 151 : {(d1.txt, 34), (d10.txt, 4), (d12.txt, 25), (d13.txt, 5), (d15.txt, 19), (d2.txt, 17)}
for line in sys.stdin:
    word, total_count, documents = line.strip().split(' : ', 2)

    # We only care about the items in our dictionary
    if word in TERMS:
        # Serialize the freqencies into workable format
        documents = [document.replace('(', '').replace(')', '') for document in documents[1:-1].split('), ')]
        documents = [tuple(document.split(', ')) for document in documents]

        # Make documents a dict for instant lookup
        documents = dict([(document, int(count)) for document, count in documents])

        tf = documents.get(FILENAME, 0)
        idf = math.log(DOCUMENT_COUNT / (1.0 + len(documents)), 10)
        tf_idf = tf * idf
        print("{0}, {1} = {2}".format(word, FILENAME, tf_idf))

Task 2 code end

Task 2 results begin
family, d1.txt = -4.81647993062	
monument, d1.txt = -0.301029995664	
horse, d1.txt = -3.3113299523	
agreement, d1.txt = -0.301029995664	
child, d1.txt = -3.3113299523	
Task 2 results end



###########################################################
# Task 3.1
###########################################################

Approach:
    1. Mapper outputs resource name and count (1)
    2. Combiner aggregates results and emits the most frequent
    2. Reducer receives n results from n combiners, picks the largest


Task 3.1 code begin
Run:
#!/usr/bin/bash
export EXC2_3_1_INPUT=/data/assignments/ex2/task2/logsLarge.txt
export EXC2_3_1_OUTPUT=/user/s1115104/data/output/exc-cw2/s1115104_task_3_1.out

hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=1 \
 -input $EXC2_3_1_INPUT \
 -output $EXC2_3_1_OUTPUT \
 -mapper mapper.py \
 -file mapper.py \
 -combiner counter.py \
 -file counter.py \
 -reducer counter.py \
 -jobconf mapred.job.name="Logs Popular L"

hdfs dfs -cat $EXC2_3_1_OUTPUT/part-* | head -n 10 > results.txt
########################################################
#!/usr/bin/python
# mapper.py
import sys

# Input:
# in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] "GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0" 200 1839
for line in sys.stdin:
    try:
        resource = line.strip().split('"')[1].split()[1]
        print("{0}\t{1}".format(resource, 1))
    except:
        sys.stderr.write("Failed to parse, skipping. Line: {0}\n".format(line.strip()))

########################################################
#!/usr/bin/python
# counter.py
import os
import sys
from collections import namedtuple

MaxCount = namedtuple('MaxCount', ['resource', 'count'])

max_counter = None
resource_counter = 0
last_resource = None


for line in sys.stdin:
    resource, count = line.strip().split('\t', 1)
    count = int(count)

    if max_counter is None:
        max_counter = MaxCount(resource, count)


    if last_resource is not None and resource != last_resource:
        # We received a new resource
        if resource_counter > max_counter.count:
            # We found a new max
            max_counter = MaxCount(last_resource, resource_counter)
        resource_counter = 0

    last_resource = resource
    resource_counter += count



if max_counter:
    if resource_counter > max_counter.count:
        # We found a new max
        max_counter = MaxCount(last_resource, resource_counter)

    print("{0}\t{1}".format(max_counter.resource, max_counter.count))
Task 3.1 code end

Task 3.1 results begin
/images/NASA-logosmall.gif	97410
Task 3.1 results end
