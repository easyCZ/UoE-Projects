Student: s1115104
Date: 24-10-2015

Output directory: /user/s1115104/data/output/exc-cw2/

###########################################################
# Task 1
###########################################################

Approach:
    1. Count words in each line and ouput a triple (word, filename, word_count)
    2. Partition output by word, sort primarily by word, secondarily by filename (alphabetically)
    3. Combiner accumulates counts by filename
    4. Reducer produces total counts for all files given a name and also outputs counts for each file

Document Sorting:
    Documents output are sorted alphabeitcally, thefeore d11.txt comes before d2.txt.

Task 1 code begin
Run:
#!/usr/bin/bash
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=16 \
 -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
 -D stream.num.map.output.key.fields=1 \
 -D num.key.fields.for.partition=1 \
 -D mapreduce.partition.keypartitioner.options=-k1,1 \
 -D mapreduce.partition.keycomparator.options="-k1,1 -k2,2" \
 -D mapreduce.map.output.key.field.separator=" " \
 -input /data/assignments/ex2/task1/large/ \
 -output /user/s1115104/data/output/exc-cw2/s1115104_task_1.out \
 -mapper mapper.py \
 -file mapper.py \
 -combiner combiner.py \
 -file combiner.py \
 -reducer reducer.py \
 -file reducer.py \
 -jobconf mapred.job.name="Inverted Index L" \
 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner

########################################################
#!/usr/bin/python
# mapper.py
import os
import sys
from collections import Counter

filename = os.environ['mapreduce_map_input_file'].split('/')[-1]

for line in sys.stdin:
    words = line.strip().split()
    if words:
        counter = Counter(words)

        for (word, count) in counter.iteritems():
            print("{0} {1} {2}".format(word, filename, count))

########################################################
#!/usr/bin/python
# combiner.py
import os
import sys

last_word = None
last_filename = None
accumulator = 0

# Input is received sorted by word and by filename
for line in sys.stdin:
    line = line.strip()

    word, filename, count = line.split(' ', 2)

    if last_word is not None and word != last_word:
        print("{0} {1} {2}".format(last_word, last_filename, accumulator))
        accumulator = 0

    last_word = word
    last_filename = filename
    accumulator += int(count)


if last_word is not None:
    print("{0} {1} {2}".format(last_word, last_filename, accumulator))
########################################################
#!/usr/bin/python
# reducer.py
import sys

word_counter = 0
last_word = None
word_documents = []


def write(keyword, total, documents):
    docs = ", ".join(["(%s, %d)" % (doc, count) for doc, count in documents])
    docs = "{%s}" % docs
    print("{0} : {1} : {2}".format(keyword, total, docs))

# Input is sorted primarily by word, secondarily by
# document name. Single pass is required without doing any
# sorting.
#
# Expected input:
#   also        d1.txt  13
#   also        d3.txt  8
#   also        d4.txt  3
#   altered     d5.txt  1
#   althorp     d5.txt  2
for line in sys.stdin:
    word, doc, count = line.split(' ', 2)

    if last_word is not None and last_word != word:
        # Cleanup after last block
        write(last_word, word_counter, word_documents)
        # Start new block
        word_documents = []
        word_counter = 0

    count = int(count)

    last_word = word
    word_counter += count
    word_documents.append((doc, count))

if last_word is not None:
    write(last_word, word_counter, word_documents)

Task 1 code end

Task 1 results begin
"'Ah, : 2 : {(d15.txt, 2)}	
"'Bad : 1 : {(d3.txt, 1)}	
"'Do : 6 : {(d15.txt, 5), (d2.txt, 1)}	
"'It : 5 : {(d15.txt, 4), (d5.txt, 1)}	
"'Listen, : 1 : {(d15.txt, 1)}	
"'Marriage? : 1 : {(d15.txt, 1)}	
"'Permit : 1 : {(d15.txt, 1)}	
"'To : 2 : {(d15.txt, 1), (d2.txt, 1)}	
"'Would : 1 : {(d15.txt, 1)}	
"According : 1 : {(d3.txt, 1)}	
Task 1 results end


###########################################################
# Task 2
###########################################################

Approach:
    Re-use index from part 1
    1. Compute index for the relevant files
    2. Use only mapper job
    2. Load the required terms file (terms.txt)
    3. For each keyword in terms.txt, compute tf-idf given the index
    4. Output only relevant keyword and their scores

Params:
    Mapper accepts two params, the total number of files and the document we are
    looking to caluclate tf-idf for.

Notes:
    Despite running it against only one file, the solution supports N documents.

Task 2 code begin
Run:
Uses code from Task1 to build the index.

#!/usr/bin/bash
export EXC2_2_INPUT=/data/assignments/ex2/task1/large/
export EXC2_2_INDEX=/user/s1115104/data/output/exc-cw2/s1115104_task_2.out/index
export EXC2_2_OUTPUT=/user/s1115104/data/output/exc-cw2/s1115104_task_2.out/score

# Clean up last run
rm ./terms.txt

# Fetch terms to look for
hdfs dfs -get /data/assignments/ex2/terms.txt ./terms.txt

# Build index
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=17 \
 -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
 -D stream.num.map.output.key.fields=1 \
 -D num.key.fields.for.partition=1 \
 -D mapreduce.partition.keypartitioner.options=-k1,1 \
 -D mapreduce.partition.keycomparator.options="-k1,1 -k2,2" \
 -D mapreduce.map.output.key.field.separator=" " \
 -input $EXC2_2_INPUT \
 -output $EXC2_2_INDEX \
 -mapper ../task1/mapper.py \
 -file ../task1/mapper.py \
 -combiner ../task1/combiner.py \
 -file ../task1/combiner.py \
 -reducer ../task1/reducer.py \
 -file ../task1/reducer.py \
 -jobconf mapred.job.name="Inverted Index L" \
 -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner

# Calculate score
hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=0 \
 -input $EXC2_2_INDEX \
 -output $EXC2_2_OUTPUT \
 -mapper "mapper.py `hdfs dfs -ls $EXC2_2_INPUT | grep -v "_SUCCESS\|Found" | wc -l` d1.txt"\
 -file mapper.py \
 -jobconf mapred.job.name="TF-IDF L" \
 -file terms.txt

hdfs dfs -cat $EXC2_2_OUTPUT/part-* | head -n 10 > results.txt

########################################################
#!/usr/bin/python
# mapper.py
import os
import sys
import math

# Requires the number of files to map to be part of the input
DOCUMENT_COUNT = int(sys.argv[1])
FILENAME = sys.argv[2]
TERMS = set()

# Build dictionary
with open('terms.txt') as f:
    for term in f:
        term = term.strip()
        TERMS.add(term)

# Input pattern:
# "And : 151 : {(d1.txt, 34), (d10.txt, 4), (d12.txt, 25), (d13.txt, 5), (d15.txt, 19), (d2.txt, 17)}
for line in sys.stdin:
    word, total_count, documents = line.strip().split(' : ', 2)

    # We only care about the items in our dictionary
    if word in TERMS:
        # Serialize the freqencies into workable format
        documents = [document.replace('(', '').replace(')', '') for document in documents[1:-1].split('), ')]
        documents = [tuple(document.split(', ')) for document in documents]

        # Make documents a dict for instant lookup
        documents = dict([(document, int(count)) for document, count in documents])

        tf = documents.get(FILENAME, 0)
        idf = math.log(DOCUMENT_COUNT / (1.0 + len(documents)), 10)
        tf_idf = tf * idf
        print("{0}, {1} = {2}".format(word, FILENAME, tf_idf))

Task 2 code end

Task 2 results begin
Lassiter, d1.txt = 0.0	
family, d1.txt = 1.14151090877	
monument, d1.txt = 0.62838893005	
horse, d1.txt = 2.57155048062	
agreement, d1.txt = 0.531478917042	
child, d1.txt = 6.524311868	
Task 2 results end



###########################################################
# Task 3.1
###########################################################

Approach:
    1. Mapper outputs resource name and count (1)
    2. Combiner aggregates results and emits the most frequent
    2. Reducer receives n results from n combiners, picks the largest


Task 3.1 code begin
Run:
#!/usr/bin/bash
export EXC2_3_1_INPUT=/data/assignments/ex2/task2/logsLarge.txt
export EXC2_3_1_OUTPUT=/user/s1115104/data/output/exc-cw2/s1115104_task_3_1.out

hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
 -D mapred.reduce.tasks=1 \
 -input $EXC2_3_1_INPUT \
 -output $EXC2_3_1_OUTPUT \
 -mapper mapper.py \
 -file mapper.py \
 -combiner counter.py \
 -file counter.py \
 -reducer counter.py \
 -jobconf mapred.job.name="Logs Popular L"

hdfs dfs -cat $EXC2_3_1_OUTPUT/part-* | head -n 10 > results.txt
########################################################
#!/usr/bin/python
# mapper.py
import sys

# Input:
# in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] "GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0" 200 1839
for line in sys.stdin:
    try:
        resource = line.strip().split('"')[1].split()[1]
        print("{0}\t{1}".format(resource, 1))
    except:
        sys.stderr.write("Failed to parse, skipping. Line: {0}\n".format(line.strip()))

########################################################
#!/usr/bin/python
# counter.py
import os
import sys
from collections import namedtuple

MaxCount = namedtuple('MaxCount', ['resource', 'count'])

max_counter = None
resource_counter = 0
last_resource = None


for line in sys.stdin:
    resource, count = line.strip().split('\t', 1)
    count = int(count)

    if max_counter is None:
        max_counter = MaxCount(resource, count)


    if last_resource is not None and resource != last_resource:
        # We received a new resource
        if resource_counter > max_counter.count:
            # We found a new max
            max_counter = MaxCount(last_resource, resource_counter)
        resource_counter = 0

    last_resource = resource
    resource_counter += count



if max_counter:
    if resource_counter > max_counter.count:
        # We found a new max
        max_counter = MaxCount(last_resource, resource_counter)

    print("{0}\t{1}".format(max_counter.resource, max_counter.count))
Task 3.1 code end

Task 3.1 results begin
/images/NASA-logosmall.gif	97410
Task 3.1 results end


###########################################################
# Task 3.2
###########################################################

Approach:
    1. Mapper outputs resource name and count (1)
    2. Combiner aggregates results and emits 10 most frequent
    2. Reducer receives 10 * n results from n combiners, picks the 10 largest


Task 3.2 code begin
Run:
#!/bin/bash

export EXC2_3_2_INPUT=/data/assignments/ex2/task2/logsLarge.txt
export EXC2_3_2_OUTPUT=/user/s1115104/data/output/exc-cw2/s1115104_task_3_2.out

hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
    -D mapred.reduce.tasks=1 \
    -input $EXC2_3_2_INPUT \
    -output $EXC2_3_2_OUTPUT \
    -mapper mapper.py \
    -combiner counter.py \
    -reducer counter.py \
    -file mapper.py \
    -file counter.py \
    -jobconf mapred.job.name="404: Cat not found."

hdfs dfs -cat $EXC2_3_2_OUTPUT/part-* | head -n 10 > results.txt


########################################################
#!/usr/bin/python
# mapper.py
import sys

# Input:
# in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] "GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0" 200 1839
for line in sys.stdin:
    try:
        host, rest = line.strip().split(' - - ')
        time, http, response = rest.strip().split('"')
        code, size = response.strip().split()
        if int(code) == 404:
            print("{0}\t{1}".format(host, 1))
    except:
        sys.stderr.write("Failed to parse, skipping. Line: {0}\n".format(line.strip()))

########################################################
#!/usr/bin/python
# counter.py
import os
import sys
import heapq


MOST_COMMON_THRESH = 10
most_common = []
heapq.heapify(most_common)

last_host = None
last_host_count = 0

for line in sys.stdin:
    host, count = line.strip().split('\t', 1)
    count = int(count)

    if last_host != host:
        # Store the new host
        # Keep a heap of most common items
        if len(most_common) >= MOST_COMMON_THRESH:
            heapq.heappushpop(most_common, (last_host_count, last_host))
        else:
            heapq.heappush(most_common, (last_host_count, last_host))

        last_host_count = 0
    
    last_host = host
    last_host_count += count

for (count, host) in heapq.nlargest(MOST_COMMON_THRESH, most_common):
    print("{0}\t{1}".format(host, count))


Task 3.2 code end

Task 3.2 results begin
dialip-217.den.mmc.com	62
155.148.25.4	44
maz3.maz.net	39
gate.barr.com	38
ts8-1.westwood.ts.ucla.edu	37
nexus.mlckew.edu.au	37
m38-370-9.mit.edu	37
piweba3y.prodigy.com	35
reddragon.ksc.nasa.gov	33
204.62.245.32	29
Task 3.2 results end


###########################################################
# Task 3.3
###########################################################

Task 3.3 code begin
Run:
#!/bin/bash

export EXC2_3_3_INPUT=/data/assignments/ex2/task2/logsLarge.txt
export EXC2_3_3_OUTPUT=/user/s1115104/data/output/exc-cw2/s1115104_task_3_3.out

hadoop jar /opt/hadoop/hadoop-2.7.1/share/hadoop/tools/lib/hadoop-streaming-2.7.1.jar \
    -D mapred.reduce.tasks=16 \
    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \
    -D stream.num.map.output.key.fields=1 \
    -D num.key.fields.for.partition=1 \
    -D mapreduce.partition.keypartitioner.options=-k1,1 \
    -D mapreduce.partition.keycomparator.options="-k1,1 -k2,2n" \
    -D mapreduce.map.output.key.field.separator=" " \
    -input $EXC2_3_3_INPUT \
    -output $EXC2_3_3_OUTPUT \
    -mapper mapper.py \
    -combiner combiner.py \
    -reducer reducer.py \
    -file mapper.py \
    -file combiner.py \
    -file reducer.py \
    -jobconf mapred.job.name="How long did you stay around? EXC2 3.3 L" \
    -partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner

hdfs dfs -cat $EXC2_3_3_OUTPUT/part-* | head -n 10 > results.txt

########################################################
#!/usr/bin/python
# mapper.py
import sys
from datetime import datetime


def parse_datetime(dt):
    timestamp = dt.split()[0]
    return datetime.strptime(timestamp, '%d/%b/%Y:%H:%M:%S')


def get_timestamp(dt):
    return (dt - datetime(1970, 1, 1)).total_seconds()

# Input:
# in24.inetnebr.com - - [01/Aug/1995:00:00:01 -0400] "GET /shuttle/missions/sts-68/news/sts-68-mcc-05.txt HTTP/1.0" 200 1839
#
# Output:
# in24.inetnebr.com     <unix timestamp>
for line in sys.stdin:
    try:
        host, rest = line.strip().split(' - - ')
        time, _ = rest.strip().split('"', 1)
        time = time.replace('[', '').replace(']', '').strip()
        timestamp = get_timestamp(parse_datetime(time))

        # Maintain the actual time information as well for output reasons, timestamps lose timezone info
        print("{0} {1} {2}".format(host, int(timestamp), time))

    except:
        sys.stderr.write("Failed to parse, skipping. Line: {0}\n".format(line.strip()))

########################################################
#!/usr/bin/python
# range.py
import sys
from collections import namedtuple

AccessRange = namedtuple('AccessRange', ['host', 'time_from', 'time_to'])

# Input:
# 1.ts2.mnet.medstroms.se 807657230 05/Aug/1995:17:13:50 -0400
# 1.ts2.mnet.medstroms.se 807657240 05/Aug/1995:17:14:00 -0400
# 1.ts2.mnet.medstroms.se 807657248 05/Aug/1995:17:14:08 -0400
# 1.ts2.mnet.medstroms.se 807657248 05/Aug/1995:17:14:08 -0400
# 11.ts1.mnet.medstroms.se 807637425 05/Aug/1995:11:43:45 -0400
#
# Output:
# 1.ts2.mnet.medstroms.se 807657230 05/Aug/1995:17:13:50 -0400
# 1.ts2.mnet.medstroms.se 807657248 05/Aug/1995:17:14:08 -0400
# 11.ts1.mnet.medstroms.se 807637425 05/Aug/1995:11:43:45 -0400
#
# Approach:
#   * Input is sorted by host and secondarily sorted by UNIX timestamp
#   * Keep track of the first visit and find the last row for the same host
#   * Output the start time and optionally the last time

def write(acs_range):
    # Write <hostname> <timestamp> <datetime>
    print('{0} {1} {2}'.format(acs_range.host, acs_range.time_from[0], acs_range.time_from[1]))
    # Start and end is different
    if acs_range.time_to and acs_range.time_from[0] != acs_range.time_to[0]:
        print('{0} {1} {2}'.format(acs_range.host, acs_range.time_to[0], acs_range.time_to[1]))

access_range = None

for line in sys.stdin:
    host, timestamp, datetime = line.strip().split(' ', 2)

    if not access_range:
        # Initialize
        access_range = AccessRange(host, (timestamp, datetime), None)

    if access_range.host != host:
        write(access_range)

        # New host row
        access_range = AccessRange(host, (timestamp, datetime), None)
    else:
        # The same host again, update end time
        access_range = AccessRange(access_range.host, access_range.time_from, (timestamp, datetime))

# Write the final
if access_range:
    write(access_range)
########################################################
#!/usr/bin/python
# range.py
import sys
from collections import namedtuple

AccessRange = namedtuple('AccessRange', ['host', 'time_from', 'time_to'])

# Input:
# 1.ts2.mnet.medstroms.se 807657230 05/Aug/1995:17:13:50 -0400
# 1.ts2.mnet.medstroms.se 807657248 05/Aug/1995:17:14:08 -0400
# 11.ts1.mnet.medstroms.se 807637425 05/Aug/1995:11:43:45 -0400
#
# Output:
# <hostname>    <seconds from first to last visit>  [<first_visit>]   [<last_visit>]
# Or when only a single visit:
# <hostname>    0  [<first_visit>]   [<first_visit>]
#
# Ie:
# 1.ts2.mnet.medstroms.se 18 [05/Aug/1995:17:13:50 -0400] [05/Aug/1995:17:14:08 -0400]
# 11.ts1.mnet.medstroms.se 0 [05/Aug/1995:11:43:45 -0400] [05/Aug/1995:11:43:45 -0400]
#
# Approach:
#   * Input is sorted by host and secondarily sorted by UNIX timestamp
#   * Keep track of the first visit and find the last row for the same host
#
# Notes:
#   * The amount of data output is slightly more than just the time difference
#     but knowing the host and the length of the stay as well as the timestamps
#     is more useful and better parseable

def write(acs_range):
    delta = 0
    datetime_from = acs_range.time_from[1]
    datetime_to = acs_range.time_from[1]

    if acs_range.time_to:
        delta = int(abs(int(acs_range.time_to[0]) - int(acs_range.time_from[0])))
        datetime_to = acs_range.time_to[1]

    print('{0}\t{1}\t{2}\t{3}'.format(acs_range.host, delta, datetime_from, datetime_to))


access_range = None

for line in sys.stdin:
    host, timestamp, datetime = line.strip().split(' ', 2)

    if not access_range:
        # Initialize
        access_range = AccessRange(host, (timestamp, datetime), None)

    if access_range.host != host:
        write(access_range)

        # New host row
        access_range = AccessRange(host, (timestamp, datetime), None)
    else:
        # The same host again, update end time
        access_range = AccessRange(access_range.host, access_range.time_from, (timestamp, datetime))

# Write the final
if access_range:
    write(access_range)
Task 3.3 code end

Task 3.3 results begin
02-17-05.comsvc.calpoly.edu	182	28/Aug/1995:11:02:36 -0400	28/Aug/1995:11:05:38 -0400
10md423.uni-duisburg.de	3497	15/Aug/1995:05:11:33 -0400	15/Aug/1995:06:09:50 -0400
128.100.178.11	2	11/Aug/1995:08:48:25 -0400	11/Aug/1995:08:48:27 -0400
128.100.80.97	83	16/Aug/1995:12:16:50 -0400	16/Aug/1995:12:18:13 -0400
128.103.196.21	243	21/Aug/1995:22:40:15 -0400	21/Aug/1995:22:44:18 -0400
128.104.225.29	135	25/Aug/1995:18:31:19 -0400	25/Aug/1995:18:33:34 -0400
128.114.23.37	181	12/Aug/1995:17:04:38 -0400	12/Aug/1995:17:07:39 -0400
128.119.50.139	3509	03/Aug/1995:13:27:57 -0400	03/Aug/1995:14:26:26 -0400
128.120.71.79	2	13/Aug/1995:22:13:44 -0400	13/Aug/1995:22:13:46 -0400
128.138.243.150	0	03/Aug/1995:13:48:34 -0400	03/Aug/1995:13:48:34 -0400
Task 3.3 results end